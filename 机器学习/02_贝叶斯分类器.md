outline
1. 相关知识
2. 贝叶斯决策
3. 朴素贝叶斯分类器
4. EM算法

## 相关知识

### 极大似然估计:

(1). 极大似然估计在求什么？

  是根据输入的赝本的性质最可能产生当前预测结果的模型的参数进行估计。并不是在求概率，而是在对分类器的参数进行估计。
  
(2). 极大似然估计的前提是什么？
  
  各个样本之间是独立同分布的，这样才能保证使用统一的概率分布模型，对参数进行学习得到一个准确的分类器。
  
(3). 极大似然估计怎么求？

  为了使模型对某个样本预测得到的预测标签的概率最大，列出最大似然函数，求导，导数最大时的样本的参数就是当前模型的最优参数，即极大似然估计要求的结果。

### 贝叶斯定理：
(1). 贝叶斯定理在解决什么问题？

  贝叶斯定理为求解条件概率提供方法，在B事件发生的情况下，A事情发生的概率。

(2). 贝叶斯定理公式：
  
  P(A|B)= P(A) * P(B|A)/P(B)
 
(3). 如何从实际意义上理解贝叶斯公式？

  根据新得到的信息（P(B|A)/P(B)，可能性函数）来对先验概率（P(A)，在不知道相关信息的时候，事件A发生的可能性）进行修正。我们就可以知道，在新的信息发生的情况下，事件A发生的概率是多少。

###模型分类
判别式模型：输入一个训练样本，给出对样本的预测结果。学习一个模型来拟合P(C|X)，其中X是输入样本，C是判决结果，P使我们要学习的模型。
生成式模型：学习样本特征和类别之间的关系。

## 贝叶斯决策（多分类）
### 思想
  贝叶斯决策是寻找一个分类模型的思想，使得模型在单一样本的条件风险最小，总体上在所有的样本中的期望损失最小。
  而真正寻找这个决策器的方式是极大似然估计。
1. 条件风险：单一样本的误判为某一类的损失*在已知当前样本属性的条件下误判为某一类的条件概率，对有所的可能的误判的类别求和。

    R(x) = \sum_{i=1}^{N}{\lambda_{i}*P(i|X)}
  
    N:是总共的可能的分类结果数目，多分类的标签数
2. 总体风险：

  \sum_{i=1}^{M}{R(x_{i})}
  M:是总体的样本数目
  
3. 贝叶斯最优分类器:找到一个能使总体风险最小的分类器。（对每一个样本，尽可能的在正确的标签上预测的概率大，而在其他标签上预测的概率小）。这时对应的风险为贝叶斯风险。
## 朴素贝叶斯
1. 限制条件：样本的各个特征之间是独立的。所以P(X|C)=\mul_{i}^{w}P(X_{i}|C),其中w是X的特征的维度。
