Outline:
1. 相关知识点
2. 选择非叶子节点的策略
3. 构建决策树的方法(非叶子节点的属性选择,决策树生成,决策树剪枝)
4. 面试常考

## 相关知识点
### 熵
1.概念:热力学中熵指物体的混乱程度.数值越大越混乱,数值越小越有序.
      信息领域:数值越大表示结果越不确定,数值越小表示结果越确定.
      
2.物理理解:表示为了使情况越确定(熵减小),需要添加的信息越多.各种情况以均匀分布出现时,是信息熵最大的情况. 

3.计算公式:

      H(x)=-\sum_{i=1}^{N}{p_{i}*log_{2}(p_i)}
  
### 条件熵

1.概念:在已知随机变量X的情况下Y的不确定程度.

2.物理理解:当X为某个特定的取值时,Y在当前情况下的总数和分布情况也会发生变化.条件熵表示当X为某个他可能的取值时,Y的熵,然后对所有的X的可能取值下的Y的熵求和.

3.计算公式:

      H(Y) = \sum_{x}{p(x)*H(Y|X)}
      
      H(Y|X) = -sum_{y=1}^{N}{p(Y|X)*log(p(Y|X))} = -sum_{y=1}^{N}{p(X,Y)/p(X)*log(p(Y|X))}
      
      H(Y) = -\sum_{x}{p(x)*-sum_{y=1}^{N}{p(X,Y)/p(X)*log(p(Y|X))}}
      
      H(Y) = -\sum_{x}sum_{y}{p(X,Y)*log(p(Y|X))}
      
      x:是特征X的所有可能取值,N是当特征为X为某一个可能取值时,样本数量.
      
## 选择非叶子节点的策略
### 信息增益(ID3)
1.概念:随机变量的信息熵在特征X给定的情况下的条件熵之差.

2.物理理解:得知信息X而使Y不确定性减少的程度.为了使熵减小一定的程度,需要加入的信息,信息增益就是熵减小的程度,信息带来的增益.熵减.

3.计算公式:

      G(Y,X) = H(Y) - H(Y|X)

4.关于信息增益与特征X类别:

   理论上来说,如果采集的样本的充足,满足大数定理,那么特征的类别数是不会影响信息增益的大小的.但是我们在收集特征和样本的时候,往往并不能做到.这就导致了如果特征X的取值数量越多,那么在当前特征X的某个取值下,Y就越纯,条件熵很小,信息增益很大.进而造成了特征X的可能取值越多,信息增益就越大的结果.
      
   举个例子,如果我们划分Y男女,按照特征X为身份证号进行划分.但是每个样本都有自己的唯一的身份证号,每个身份证号又只对应一个性别.这样信息增益很高,但是这样的划分毫无意义.
      
### 信息增益比(C4.5)
1.概念:通过引入特征X的信息熵对信息增益进行修正.

2.物理解释:如果特征X的可能取值越多,那么X的信息熵就越大,则它对应的信息增益率就越小,使得信息增益率会对特征X可能取值少的特征有偏好.

3.公式:

      G_ratio = G(Y,X)/H(X)

### 基尼指数(CART-classification and regression tree)
1.基尼值

(1).物理解释:
   从数据集中随机的取两个样本,他们的标签不一致的概率.
   基尼值越大,表示当前样本集纯度越低,基尼值越小,表示当前样本集纯度越高.
   
(2).公式:

      Gini(X) = \sum_{i=1}^{N}\sum_{j \notequal i}{p_{i}*p_{j}}
      
      N:当前集合中的样本总数
 
2.基尼指数

 N:样本总数
 
 N_x:当特征值为某一特定取值时,样本的数目
 
 (1).公式:
 
      Gini_index = \sum_{x=1}^{X}{N_x/N}Gini{Y}

 (2).选择基尼指数较小的特征作为非叶子节点的特征

##面试常考
